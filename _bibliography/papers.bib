---
---

@inproceedings{somashekar2024oppertune,
  author    = {Gagan Somashekar* and Karan Tandon* and Anush Kini and Chieh-Chun Chang and Petr Husak and Ranjita Bhagwan and Mayukh Das and Anshul Gandhi and Nagarajan Natarajan},
  title     = {{OPPerTune}: {Post-Deployment} Configuration Tuning of Services Made Easy},
  booktitle = {21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)},
  year      = {2024},
  isbn      = {978-1-939133-39-7},
  address   = {Santa Clara, CA},
  pages     = {1101--1120},
  url       = {https://www.usenix.org/conference/nsdi24/presentation/somashekar},
  video     = {https://www.youtube.com/watch?v=c9snEu5Wv-o},
  pdf       = {https://www.usenix.org/system/files/nsdi24-somashekar.pdf},
  publisher = {USENIX Association},
  month     = apr,
  abstract  = {Real-world application deployments have hundreds of interdependent configuration parameters, many of which significantly influence performance and efficiency. With today's complex and dynamic services, operators need to continuously monitor and set the right configuration values (configuration tuning) well after a service is widely deployed. This is challenging since experimenting with different configurations post-deployment may reduce application performance or cause disruptions. While state-of-the-art ML approaches do help to automate configuration tuning, they do not fully address the multiple challenges in end-to-end configuration tuning of deployed applications. This paper presents OPPerTune, a service that enables configuration tuning of applications in deployment at Microsoft. OPPerTune reduces application interruptions while maximizing the performance of deployed applications as and when the workload or the underlying infrastructure changes. It automates three essential processes that facilitate post-deployment configuration tuning: (a) determining which configurations to tune, (b) automatically managing the scope at which to tune the configurations, and (c) using a novel reinforcement learning algorithm to simultaneously and quickly tune numerical and categorical configurations, thereby keeping the overhead of configuration tuning low. We deploy OPPerTune on two enterprise applications in Microsoft Azure's clusters. Our experiments show that OPPerTune reduces the end-to-end P95 latency of microservice applications by more than 50% over expert configuration choices made ahead of deployment. The code and datasets used are made available at https://aka.ms/OPPerTune.},
  selected  = {true}
}

@inproceedings{anonymous2024reward,
  title     = {Reward Copilot for {RL}-driven Systems Optimization},
  author    = {Karan Tandon and Manav Mishra and Gagan Somashekar and Mayukh Das and Nagarajan Natarajan},
  booktitle = {NeurIPS 2024 Workshop on Machine Learning for Systems},
  year      = {2024},
  month     = dec,
  url       = {#},
  pdf       = {#},
  abstract  = {Systems optimization problems such as workload auto-scaling, kernel parameter tuning, and cluster management arising in large-scale enterprise infrastructure are becoming increasingly RL-driven. While effective, it is difficult to set up the RL framework for such real-world problems --- designing correct and useful reward functions or state spaces is highly challenging and needs a lot of domain expertise. Our proposed novel reward co-pilot solution can help design suitable and interpretable reward functions guided by client-provided specifications for any RL framework. Using experiments on standard benchmarks as well as systems-specific optimization problems, we show that our solution can return reward functions with a certain (informal) feasibility certificate in addition to pareto-optimality.},
  selected  = {true}
}
